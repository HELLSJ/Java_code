# 1、分布式系统基础

## 定义与特征

**==理解分布式系统的核心是网络上的资源共享，以及其区别于集中式系统的特点。==**

三种网络计算机系统的组织形式不同，节点代表计算机系统，边代表两个节点间的通信链路：
![[Pasted image 20251112153731.png]]
### 集中式系统

单个中心节点连接多个外围节点，所有通信和控制都通过中心点进行。若中心节点故障，整个系统将陷入瘫痪。

Centralized System: A single central node connected to several peripheral nodes. All communication and control pass through a central point. **If the central node fails, the entire system is disrupted.**
### 去中心化系统

多个中心节点各自连接一组外围节点，中心节点之间存在部分连接。系统存在多个中心点，每个中心点负责网络的一个子集。单个中心节点故障不一定导致整个系统崩溃，比集中式系统更具韧性，但仍存在关键故障点。

Decentralized System: Several central nodes each connected to their own set of peripheral nodes, with some connections between the central nodes. There are multiple central points, each responsible for a subset of the network. Failure of one central node does not necessarily disrupt the entire system. **More resilient than a centralized system but still has critical points of failure**

### 分布式系统

多个节点以网状结构互联，无明确中心点。每个节点与多个其他节点连接，数据传输可通过多条路径进行。因无单点故障风险，系统具备高度韧性，适用于对可靠性和冗余性要求较高的大规模网络。
**Multiple nodes interconnected in a web-like structure ==without a clear central point**==. **Each node is connected to multiple other nodes**, facilitating multiple paths for data to travel. **Highly resilient** as there is no single point of failure. Ideal for large-scale networks where reliability and redundancy are crucial.

### 支持资源共享

- 成本效益：共享大型存储系统比个人单独购置和维护更经济。
- 协作便利：连接用户和资源，促进信息共享与协作，例如互联网的文件共享协议、OneDrive、iCloud Drive、谷歌云端硬盘（Google Drive）、Dropbox 等云存储服务。
- 典型案例：
    - 群件（Groupware）：支持视频通话、文档共享等协作功能，在新冠疫情期间远程办公场景中发挥重要作用。
    - 对等网络（P2P）：如 BitTorrent，用户可相互共享文件片段，高效分发大型文件（如电影、软件更新），降低单台服务器负载。
    - 电子邮件系统：如 Gmail、Outlook，由第三方服务提供商管理存储、垃圾邮件过滤和安全保障，无需组织自行维护服务器。
    - 内容分发网络（CDNs）：如 Cloudflare、Akamai，将网站内容分布在全球多个服务器，用户从最近的服务器获取内容，提升加载速度和可靠性。
## 关键属性

**==掌握可靠性和容错性的概念，即系统在发生故障时仍能继续正确运行的能力。**==
### 可靠性 reliability

**定义：系统在一段时间内连续无故障运行的特性 a system can run continuously without failure**

Reliability: It means the system can **==run continuously without failing==**. Unlike availability, ==**reliability is measured over a period of time**==. So, a highly reliable system is one that can keep working without interruptions for a long stretch of time. For example, if a system goes down for a millisecond every hour, it has high availability (over 99.9999%) but is still unreliable. However, a system that never crashes but shuts down for two weeks every August is highly reliable but has only 96% availability. 

（可靠性：指系统能够连续运⾏⽽不发⽣故障。与可⽤性不同，可靠性是基于⼀段时间来衡量的。因此，⾼可靠性系统是指能够在很⻓⼀段时间内不间断⼯作的系统。例如，若⼀个系统每⼩时停机 1 毫秒，其可⽤性极⾼（超过 99.9999%），但可靠性仍不⾜；⽽⼀个从不崩溃但每 年 8 ⽉停机两周的系统，可靠性很⾼，但可⽤性仅为 96%。两者的区别显⽽易⻅。）
### 容错性 fault tolerance

**这一策略着重于确保系统在出现故障时仍能正常运行。这就像拥有备份系统或使用纠错码来确保一切顺利运转一样。** This startegy focuses on ensuring **the system can still function correctly even when faults occur**. This is like having backup systems or using error-correcting codes to keep things running smoothly.

![[Pasted image 20251228160302.png]]

- 故障相关概念：

| 术语          | 描述          | 示例    |
| ----------- | ----------- | ----- |
| 故障（Failure） | 组件未达到规格要求   | 程序崩溃  |
| 错误（Error）   | 可能导致故障的组件缺陷 | 编程漏洞  |
| 缺陷（Fault）   | 错误的原因       | 程序员疏忽 |
## 可扩展性

**==理解其瓶颈，如CPU、磁盘I/O和网络延迟，并认识到带宽并非无限。==**


- **定义**：In most cases, scalability problems in distributed systems appear as performance problems caused by limited capacity of servers and network. ⼤多数分布式系统的可扩展性问题表现为服务器和⽹络容量有限导致的性能问题
- 核心维度：
    - 规模可扩展性：支持更多用户或进程。
    - 地理可扩展性：节点间最大距离增加。
    - 管理可扩展性：支持更多管理域。

- 规模可扩展性问题根源（**集中式方案**）：
    - 计算能力有限 computational capacity（受 **CPU 限制**）。
    - 存储能力有限 storage capacity（含 **CPU 与磁盘间的传输速率限制**）。
    - 网络**带宽限制** bandwith is not infinite network between the user and the centralized service（用户与集中式服务间的网络瓶颈）。
![[Pasted image 20251228005654.png]]

**注意下面是错误的假设！！！**
![[Pasted image 20251227194148.png]]
# 2、系统架构与通信模型

- 分层架构（Layered architectures）
- 面向服务架构（Service-oriented architectures）
- 发布 - 订阅架构（Publish-subscribe architectures）

## 面向服务的架构 Service-Oriented Architecture

**==理解其核心原则，包括松耦合、可重用性和自主性，而非集中式处理。==**
### 定义

面向服务架构（SOA）是一种设计模式，用于构建**通过协议向其他应用提供服务的分布式系统**，不受编程语言或平台限制。
### 服务的定义

服务是具有明确定义、独立完整的功能单元，可与其他服务交换信息，不依赖其他服务的状态，采用松耦合、基于消息的通信模型与应用程序及其他服务交互。

### 核心原则

- 松耦合 **==loose coupling==**：用于满足可扩展性、灵活性和容错性需求，旨在最小化依赖关系 ==**minimize dependencies**==。依赖越少，一个系统的修改或故障对其他系统的影响就越小 When there are fewer dependencies, modifications to or faults in one system will have fewer consequences on other systems。
- 可重用性 **reusability**：服务设计为可在不同场景中重用 **reuse in different scenarios**，提升系统效率和一致性。
- 自主性 **autonomy**：服务独立运行，自主控制自身功能。 Services are **independent** and **control their own functionality**

## 通信范式

### 发布-订阅模型 Publish-subscribe architecture （理解其是一种基于事件的协调，实现了时间耦合但引用解耦）

One key aspect of publish-subscribe systems is that **==communication happens based on the events that a subscriber is interested in==**.
(发布 - 订阅系统的核⼼是基于订阅者感兴趣的事件进⾏通信。)
#### 两种协调模型：时间耦合与引用耦合

- **时间耦合**：必须同时在线
- **时间解耦**：不用同时在线
- **引用耦合**：必须知道对方是谁
- **引用解耦**：不知道对方是谁也能通信

|                              | 时间耦合 temporally coupled | 时间解耦 temporally decoupled |
| ---------------------------- | ----------------------- | ------------------------- |
| 引用耦合 referentially coupled   | 直接协调                    | 邮箱协调                      |
| 引用解耦 referentially decoupled | 基于事件的协调                 | 共享数据空间协调                  |

- 直接协调（时间耦合 + 引用耦合）：进程需相互知晓且同时活跃，例如打电话（需知道对方号码，且对方需接听）。
- 邮箱协调（时间解耦 + 引用耦合）：进程无需同时活跃，可将消息放入邮箱，但需知道邮箱位置。
- **基于事件的协调 event-based coordination（时间耦合 + 引用解耦）：进程不直接知晓彼此，仅发布事件通知，其他进程可订阅。==理想情况下，通知仅发送给订阅者，但订阅者需在通知发送时处于活跃状态 Ideally, notifications reach only those who subscribed to them, but subscribers need to be active when the notification is sent.==。** ==（发布-订阅模型的本质）==
- 共享数据空间协调（时间解耦 + 引用解耦）：进程通过元组（类似数据库行的结构化数据记录）通信，可向共享数据空间写入元组或通过搜索模式检索元组，支持关联搜索（指定感兴趣字段的值，返回匹配元组）。
![[Pasted image 20251228161934.png]]
### 远程过程调用 Remote Procedure Call

**==远程过程调用（作为一种紧密耦合的通信方式）。==**

远程过程调用(RPC)：请求通过本地过程调用发送，封装为消息处理后通过消息响应，并将结果作为调用返回。Requests are sent through local procedure call, packaged as message, processed, responded through message, and result returned as return from call

RPC要求调用者和被调用者同时处于运行状态。
**RPCs require caller and callee to be up and running at the same time.** 

RPC 允许⼀台机器上的客户端调用另⼀台机器上实现的过程。
An RPC allows a client on one machine to make a call to a procedure implemented on another machine. 


![[Pasted image 20251227151258.png]]
NFS 中的所有客户端与服务器之间的通信均通过远程过程调用(RPC)实现。**RPC使得一台机器上的客户端能够向另一台机器上实现的过程发出调用**。NFS 客户端将 NFS 文件系统操作以远程过程调用的形式实现并发送给服务器。VFS接口所提供的操作可能与 NFS客户端所提供的操作有所不同。VFS的理念在于隐藏不同文件系统之间的差异。

- 客户端像调用本地函数一样调用远程函数
- 实际过程：
    - 序列化 → 发送 → 远程执行 → 返回结果
- 必须知道：
    - **服务器地址**（referential coupling）
    - **方法名、参数格式**（接口耦合）

因此 RPC 具有：
1. 时间耦合（双方必须在线）
2. 引用耦合（客户端必须知道服务器）
3. 强耦合性（依赖固定接口与同步调用）

![[Pasted image 20251228162248.png]]
## 网络基础

**==掌握TCP三次握手过程，特别是SYN-ACK报文段代表服务器确认并发起序列号。==**
### TCP 三次握手

TCP 通过 “带重传的肯定确认（PAR）” 实现可靠通信，传输层的协议数据单元称为段（segment）。发送方会重发数据单元直到收到确认，接收方收到损坏的段会丢弃，发送方需为重传未收到确认的数据单元。
#### 三次握手步骤

1. 同步（SYN）：**客户端向服务器**发送包含同步**序列号（SYN）** 的段，告知服务器即将开始通信及起始序列号（如 seq=x）。
2. **服务器**收到客户端的 SYN 后，会用一个 **SYN+ACK** 报文段做出**回应**，其中包含：
	- **ACK**：确认 acknowledge 收到客户端的 SYN（“你的序列号我收到了”）；
	- **SYN**：同时告诉客户端 **服务器自己的初始序列号** what sequence number it is likely to start the segments with（服务器也需要同步序列号来开始通信）。
3. 确认（ACK）：**客户端确认服务器**的响应（ACK=y+1），双方建立可靠连接，开始实际数据传输。

![[Pasted image 20251227155503.png]]
# 3、操作系统与虚拟化技术

## 并发
### 线程

**==理解它是共享CPU时间的最小软件执行单元==**
#### 核心思想

- **线程**：与进程类似，线程独立运行自身代码，但不追求高并发性。**线程系统仅保留让多个线程共享 CPU 所需的必要信息，相当于可执行一系列指令的最小软件处理器 thread systems keep just the necessary information to let multiple threads share a CPU. ==A thread is like a minimal software processor that can run a series of instructions==.** 保存线程上下文即停止当前执行，并保留后续恢复所需的所有数据。
- **进程**：本质是操作系统虚拟处理器正在执行的程序，多个进程可同时共享同一 CPU。软件中的进程上下文类似硬件中的处理器上下文，进程是一个软件处理器，可运行一个或多个线程。运行线程即在该线程的上下文中执行指令。

线程是进程的更小组成部分，能提升分布式系统的效率和可管理性，通过聚焦维持系统平稳运行的必要功能，在不增加系统负载的前提下提升性能。
### 上下文切换 context switching

**==理解其本质是在进程/线程转换时保存和恢复状态的过程==**
#### 核心思想

**保存上下文意味着停止当前的执行过程，同时保留所有后续恢复执行所需的数据**。
==**stopping the current execution and keeping all the data needed to resume it later.**==

作⽤：helps the operating system manage and switch between different tasks efficiently.
（上下⽂各⾃帮助操作系统⾼效管理和切换不同任务。）

**上下文切换，就是在进程或线程切换运行时：**

1. **保存当前执行实体（进程/线程）的完整状态（context）**
2. **加载下一个需要执行的实体的状态**
3. **使 CPU 能够继续从原来的位置执行**

## 虚拟化与容器化
### 虚拟化 Virtualisation

**==理解其可以帮助遗留软件在现代系统上运行==**
### 虚拟化简介

- 线程和进程本质上是同时执行多个任务的方式，让程序看起来是并行运行的。在单处理器（或单核）计算机上，这种并行执行实际上是一种错觉 —— 由于只有一个 CPU，同一时间只能运行一个线程或进程的一条指令，但通过快速切换线程和进程，营造出并行的效果。
- **这种利用一个 CPU 模拟多个 CPU 的思想可扩展到其他资源，称为资源虚拟化。==This idea of using one CPU to pretend there are more extends to other resources too, which we call resource virtualisation==.**
### 虚拟化原理：模拟接口

- 实际中，每个分布式计算机系统都为高层软件提供编程接口，这些接口可以是 CPU 提供的基本指令集，也可以是大量应用程序编程接口（API）。
- **虚拟化的核心是扩展或替换现有接口，使其表现得像另一个系统。** **==virtualisation is about extending or replacing an existing interface to make it behave like another system==**
![[Pasted image 20251228162909.png]]


While hardware and low-level software evolved quickly, higher-level software like middleware and applications tended to be more stable. **This created a situation where old software couldn't keep up with the platforms it depended on. Virtualisation helps here by allowing old interfaces to run on new platforms, making these new platforms immediately compatible with a lot of existing software.**

尽管硬件和底层软件发展迅速，但中间件和应⽤程序等⾼层软件往往更稳定。**这导致旧软件⽆法跟上其所依赖的平台，虚拟化通过允许旧接口在新平台上运行解决了这⼀问题**，使这些新平台能够⽴即与⼤量现有软件兼容。
### 容器 containers

**==理解它是一个轻量级的、隔离的软件环境==** **(lightweight isolated software environment)**

虚拟机允许需要特定操作系统环境（包括指令集和操作系统）的应用程序在不同平台上运行。然而，许多应用程序的指令集和操作系统稳定，但**依赖特定的库和支持软件**。在这种情况下，我们希望不同应用程序并排运行，每个应用程序拥有自己的环境，且互不干扰，这就是容器的用武之地。

容器的本质：

- **容器可视为一组二进制文件（也称为镜像）的集合，共同构成应用程序运行的软件环境。Think of a container as a collection of binaries (also called images) that together make up the software environment for running applications.**
- 最简单的理解方式是想象用户登录 Unix 系统时看到的内容：多个标准目录，包含可执行程序、库、文档等。
- Virtual machines let you run applications that need a specific operating environment, including its instruction set and operating system, across different platforms. However, **==many applications==** are stable with their instruction set and operating system but ==**rely on specific libraries and support software**==. In these cases, we want different applications to run side-by-side, each with its own environment, without them noticing each other. This is where containers come in.（虚拟机允许你在不同平台上运行需要特定操作系统环境（包括其指令集和操作系统）的应用程序。然而，许多应⽤程序的指令集和操作系统稳定，但依赖于特定的库和⽀持软件。在这种情况下，我们希望不同的应用程序并排运⾏，每个应⽤程序都有⾃⼰的环境，⽽不会相互察觉，这就是容器的用武之地）
- 容器实际上虚拟化了应用程序的软件环境，允许其在自己的隔离空间中运行，同时与其他容器共享底层系统，便于管理和部署应用程序，避免冲突。 A container effectively virtualizes the software environment for an application, allowing it to run in its own **isolated space** even though it's **sharing the same underlying system with other containers**. This makes it much easier to manage and deploy applications without conflicts.

简单容器实现的局限性：

- 不同容器中的应用程序和进程需要相互隔离。
- 为每个容器复制整个环境效率极低，因为许多库和资源在容器间共享。
- 宿主容器的操作系统需要有效控制资源。

### 容器的核心机制

容器通过以下机制实现高效、隔离和可管理：

- 命名空间（Namespaces）：容器中的进程集合拥有自己的标识符视图。
- 联合文件系统（Union file system）：将多个文件系统以分层方式组合，仅最高层允许写操作（且属于容器的一部分）。
- 控制组（Control groups）：可对进程集合施加资源限制。
# 4、数据复制与一致性模型
## 复制 Replication

**==理解其主要目的是提高容错性。==**
### 复制的原因

假设我们对系统的特定部分（包含代码和数据）进行复制，核心目的有两点：

1. ==**提升容错性 Increase fault tolerance**==：**若文件系统完成复制，即便其中一个副本崩溃，也可切换至其他副本继续工作；多副本还能有效防范数据损坏**。例如，一个文件存在三个副本时，若某次写入操作失败，可采信另外两个副本的结果。If a file system is replicated, you can keep working even if one copy crashes by switching to another. **Multiple copies also help protect against data corruption**
2. ==**优化性能 Performance**==：当系统需要在规模或地理范围上进行**扩展**时，复制能显著增强性能。在规模扩展场景下，若多个进程需访问某台服务器管理的数据，复制服务器并分摊负载可缓解单服务器压力；在地理扩展场景下，将数据副本部署在靠近用户的位置，可缩短访问时延、提升用户感知性能，但可能会消耗更多网络带宽用于副本同步。 **Replication enhances performance when a system needs to scale**, either in size or across a wide area. **For size**, if more processes need to access data managed by one server, **replicating the server and spreading the load can help**. **For geography**, **placing copies of data closer to where they are used reduces access time and improves perceived performance**. However, this might consume more network bandwidth to keep replicas updated.
## 一致性模型：

### 一致性模型的定义

一致性模型是**数据使用进程与数据存储之间的契约**：若进程遵循特定规则，数据存储将保证数据访问的正确性。例如，进程读取数据时期望获取最新变更，但在无全局时钟的分布式环境中，难以精确定义“最新写操作”，因此衍生出多种一致性模型，规定了数据读取的预期结果。
- 严格一致性模型：易用性强，但性能可能受限。
- 宽松一致性模型：性能更优，但需开发者额外处理一致性问题。
**核心权衡**：严格模型对用户友好但性能不足，宽松模型性能优异但管理复杂度高。
### 强一致性 

#### 顺序一致性 Sequential Consistency

**==顺序一致性，即所有进程看到相同的操作顺序，但不一定是实时顺序==**

**定义**：任何执行的结果，都等价于所有进程的操作按某一顺序串行执行，且每个进程的操作在该序列中保持其程序指定的顺序。
即数据存储的行为如同所有读写操作按固定顺序执行，每个进程按自身程序逻辑感知操作序列，所有进程对操作的感知顺序一致，与操作实际发生的实时时间无关。Each process sees these operations in the sequence set by its own program, even if they're on different machines. What's important here is that **all processes view the operations in the same order**, **but it doesn’t matter when they actually happened in real-time**.

![[Pasted image 20251228164406.png]]

**示例分析**：
- 符合顺序一致性的存储：进程$P1$先写$x$为$a$，进程$P2$后写$x$为$b$，进程$P3$和$P4$均先读取到$b$、后读取到$a$，所有进程感知的写操作顺序一致。
- 不符合顺序一致性的存储：$P3$感知到$x$先变为$b$再变为$a$，而$P4$先读取到$a$再读取到$b$，进程间操作顺序感知不一致，违反顺序一致性。
![[Pasted image 20251227162511.png]]
#### 因果一致性 Causal Consistency

**==所有进程必须以相同顺序观察到有潜在因果关系的写操作==**

因果一致性弱于顺序一致性，核心是区分**因果相关事件**与**并发事件**： **makes a distinction between events that might be causally related and those that are not related.**
- 若事件$b$由事件$a$引发或影响（存在因果关系），则所有进程需先感知$a$、再感知$b$。
- 无因果关系的并发写操作，不同进程可按不同顺序感知。

**规则**：==**存在潜在因果关系的写操作，必须被所有进程按同一顺序感知**；**Writes that are potentially causally related must be seen by all processes in the same order**.== 并发写操作可被不同进程按不同顺序感知。Concurrent writes may be seen in a different order by different processes.

![[Pasted image 20251228164738.png]]
![[Pasted image 20251228164753.png]]
### 弱一致性 

#### 最终一致性 Eventual consistency

**==最终一致性，即如果没有新的更新，副本最终会收敛到一致状态==**
#### 核心问题

1. 保持副本一致性的关键是确保所有冲突操作在全局按同一顺序执行。
2. 多数数据库系统中，读操作远多于写操作，核心问题是更新需多久对读进程可见。例如，CDN通常选择缓慢传播更新，因多数客户端会定向到同一副本，不易感知不一致。
3. Web场景中，网页由单一主体更新（无写写冲突），浏览器和代理的本地缓存可提升效率，但可能返回过期页面，只要用户持续访问同一缓存，该不一致通常可接受。
#### 冲突操作处理

- **读写冲突**：读与写操作并发执行时，延迟更新传播通常可接受，部分大规模分布式数据库允许一定程度的不一致，其核心特性为**最终一致性**（**若一段时间无更新，所有副本将逐渐一致**）。
- **写写冲突**：若仅少数进程可执行写操作，冲突易解决（通常选定某一写操作为“胜者”覆盖其他冲突操作），最终一致性实现成本较低；但写写冲突频发时，需通过锁等机制协调进程，这会成为大规模系统的性能瓶颈。

#### 最终一致性的定义

如果我们有一组数据存储和可能同时发生的写操作，**==当从某个时间点开始不再有新的更新时，只要之前所有的更新最终都会传播到所有副本，让它们的数据变得一致 The stores are eventually consistent when in lack of updates from a certain moment, all updates to that point are propagated in such a way that replicas will have the same data stored==**，那么我们就说这些存储系统达到了**最终一致性**

![[Pasted image 20251228165040.png]]

| 模型                                 | 顺序要求                         |
| ---------------------------------- | ---------------------------- |
| **顺序一致性 (Sequential consistency)** | 所有写必须按相同顺序被所有人看到（非常严格）       |
| **因果一致性 (Causal consistency)**     | 只有因果相关的写必须保持顺序；无关的写顺序无要求（更弱） |
| **最终一致性 (Eventual consistency)**   | 没顺序要求，只保证最终一致（更弱）            |

### 客户端为中心的一致性 Client-centric Consistency Models

The principle of a mobile user accessing different replicas of a distributed database；It guarantees a single client’s interactions with the data store, but not for concurrent accesses by different clients. 以客户端为中心的⼀致性模型保证单个客⼾端与数据存储的交互⼀致性，⽽⾮不同客⼾端的并发访问⼀致性。
#### 单调读 monotonic reads

==**单调读，即确保一个进程在读到新数据后，不会再读到旧数据==**

**定义**：如果一个进程读取了数据项x的某个值，那么该进程此后对x的任何后续读取都将始终返回相同的值或更近期的值。换言之，一旦一个进程已观测到x的某个特定值，它就再也不会看到早于该值的版本。If a process reads a value of a data item x, any later read of x by that process will always return the same value or a more recent one. **==In other words, once a process has seen a certain value of x, it will never see an older version of x.==**

**示例**：

进程 P1 首先在存储位置 L1 写入数据 x，生成数据版本 x1；随后 P1 读取了该版本的数据。与此同时，进程 P2 在存储位置 L2 基于版本 x1 生成了数据版本 x2。当 P1 的操作迁移至存储位置 L2 并再次读取数据 x 时，会获取到更新的版本 x2，而该版本至少整合了 P1 此前写入的数据内容。

![[Pasted image 20251228135533.png]]

单调读一致性被破坏。进程 P1 先在存储位置 L1 读取了版本 x1，之后又在存储位置 L2 读取了版本 x2。但进程 P2 在 L2 执行的写操作 W2 (x1|x2) 所生成的版本，并未遵循版本 x1 的内容衍生而来。因此，P1 在 L2 的读取操作，并未包含其此前在 L1 读取 x1 时对应的修改内容。
![[Pasted image 20251228135704.png]]

#### 读己所写 Read your Writes

**==读己之写一致性，即确保客户端总能读到自己已写入的最新数据**==

**定义**：每当一个进程对数据项$x$进行写入操作时，该进程随后进行的任何读取操作都会看到由该写入操作所导致的更改，无论读取操作发生在何处。换言之，在你写入内容后，你应该能够立即读取到内容，并且能看到你所进行的更改。 Whenever a process writes to a data item x, any later read by the same process will see the changes made by that write, no matter where the read happens. In other words, **after you write something, you should be able to read it right away, and it will show your changes.**

**示例**：更新网页后，浏览器需展示最新版本而非缓存的旧版本；修改密码后，需确保所有服务器同步新密码，避免用户无法立即登录。
# 5、故障模型与容错技术
## 故障类型 Failure 

#### 服务器故障类型
| 故障类型 | 服务器行为描述 |
| --- | --- |
| 崩溃故障 | 停止运行，但停止前行为正常 |
| 遗漏故障 | 无法响应请求、接收消息或发送消息（含接收遗漏、发送遗漏） |
| 时序故障 | 响应超出指定时间区间 |
| 响应故障 | 响应不正确（含值故障、状态转换故障） |
| 任意故障 | 可能在任意时间产生任意响应 |
### 崩溃故障 Crash Failure

**==进程停止响应==**

**崩溃故障**：服务器完全停止工作且不再通信(This happens when a server ==**stops working completely**== and ==**doesn't communicate anymore**==)。例如操作系统崩溃，需重启恢复（reboot），常见于个人计算机。
### 拜占庭故障 Arbitrary (Byzantine) Failure

**==进程可能产生任意（包括恶意）的响应==**

**任意（拜占庭）故障**：最严重的故障类型，服务器产生无法检测的错误输出，不可预测且易引发严重问题。任意故障往往与进程中的恶意活动有关。**==the server produces incorrect output that can't be detected as wrong.==** These failures are **unpredictable** and can **cause major issues**
### 遗漏故障 Omission failure

**==进程可能不响应或返回特定错误码==**

指服务器没有对请求作出响应的情况 This occurs when a server doesn't respond to a request。

遗漏失败分为两种类型：

1. **接收遗漏（Receive-Omission Failure）：**  
    服务器完全没有收到请求，可能是因为没有线程在监听传入请求。The **server never gets the request**, possibly because there was no thread listening for incoming requests
2. **发送遗漏（Send-Omission Failure）：**  
    服务器完成了工作，但没有把响应发送出去，可能是因为发送缓冲区已满。  
    在这种情况下，服务器必须准备好处理客户端重新发送的请求。The **server** does the work but **fails to send a response**, possibly due to an overflowed send buffer. In this case, the server might have to be ready for the client to reissue the request.

## 共识算法

**==理解Paxos算法能在多数派服务器正常工作时保证一致性==**

共识算法是分布式系统中，让集群内多个节点对某个提案 / 结果达成⼀致认知的算法，核⼼解决分布式环境下节点故障、⽹络异常导致的数据不⼀致问题。
![[Pasted image 20251228170418.png]]
![[Pasted image 20251228170506.png]]
![[Pasted image 20251228170517.png]]
![[Pasted image 20251228170553.png]]
![[Pasted image 20251228170604.png]]
![[Pasted image 20251228170620.png]]
### 假设条件

- 部分同步（partially synchronous）系统 （事实上系统甚至可能是异步的）
- 进程之间的通信可能不可靠：消息可能丢失、重复或乱序。
- 损坏的消息可以被检测到（因此会被忽略）。
- 所有操作都是确定性的（deterministic）：一旦开始执行，就能确切知道它会做什么。
- 进程可能出现崩溃故障（crash failures），但不会出现任意故障（arbitrary failures）。
- 进程之间不会串通（collude）。
### 进程类型

- **Clients（客户端）：**  发起具体操作的请求。
- **Proposers（提议者）：**  代表客户端向服务器端提出操作请求，并尝试让这些请求被接受。
- **Acceptors（接受者）：**  决定是否接受某个提案（proposed operation）。
- **Learners（学习者）：**  当多数 acceptor 同意某个提案后，学习者会执行被选定的操作。

**一般情况下：**

- 系统中会有一个主导的 proposer（称为 **leader**），推动协议达成共识。
- 如果多数 acceptor 同意某个提案，该提案就被认为是 **已选定（chosen）**。
- 一旦学习者从多数 acceptor 那里获知某提案被选定，它们就会执行该操作。

### 简化后的工作方式：

- **Leader（领导者）** 接收来自客户端的请求，并将提案发送给所有 acceptor（接受者）。
- 每个同意该提案的 acceptor 会广播一条 learn 消息。
- 当 **learner（学习者）** 从**多数派**接收到同样的 learn 消息时，它就执行这个操作。

#### 关键保障：多数派的意义

- 只要多数接受者（超过一半）正常工作，就能避免 “分裂投票”（即两个不同提案同时获得部分支持）；
- 即使部分副本崩溃或消息丢失，多数派的决策仍能被正确传递和执行，确保系统一致性。

例如：3 个副本（S1、S2、S3）中，只要 2 个正常工作，就能形成多数派。若 S1 崩溃，S2 和 S3 可基于已有的投票记录达成一致，不会因单个故障导致系统瘫痪。

Paxos 算法核⼼特性：

Paxos 是经典的分布式共识算法，其核心保障为：当集群中存在「多数派（超过半数）」的服务器正常⼯作时，算法能够保证集群最终达成唯⼀的共识，且共识结果具有不可推翻性，严格保证分布式系统的⼀致性。其原理为：任意两个多数派必然存在交集，因此不会出现冲突的共识结果，少数节点故障不会影响⼀致性的达成。

# 6、对等网络与缓存
## P2P网络

Peer-to-peer architectures：Processes are all equal: the functions that need to be carried out are represented by every process ⇒**each process will act as a client and a server at the same time** (i.e., acting as a servant).

所有进程地位平等，需执⾏的功能由每个进程共同承担，即每个进程同时作为客户端和服务器（服务者）。
### 结构化P2P

**==通常使用哈希函数来分配数据键==**
#### 核心本质

使用无语义索引 semantic-free index：每个数据项与唯一密钥关联，密钥作为索引（通常通过哈希函数生成：`key(数据项)=hash(数据项的值)`）。P2P 系统负责存储（密钥，值）对。

#### 分布式哈希表（DHT）

- 系统存储（密钥，值）对，每个节点分配一个来自所有可能哈希值集合的标识符。
- 每个节点负责存储与特定密钥子集相关的数据。Each node is responsible for storing data associated with a specific subset of keys.
- 核心任务：**==通过密钥查找数据项，高效将密钥映射到存储对应数据的节点。look up data items by their keys and efficiently maps a key to the node that stores the corresponding data==**
- 系统的结构化拓扑至关重要：任何节点均可接收密钥查找请求，并将请求路由到负责该密钥的节点。Any node can be asked to look up a key, and it routes the request to the node responsible for that key.
### 超级节点

**==在层次化P2P中，它对其他peers来说扮演着服务器的角色==**
#### 分层组织的对等网络（ Hierarchically Organized P2P Networks）

##### 核心本质

在纯粹的对等网络（P2P）中，有时候打破对称性是有意义的：

- 在无结构化的 P2P 系统中进行搜索时，引入 **索引服务器（index servers）** 可以提升性能。
- 决定将数据存储在哪里，通过 **代理（brokers）** 往往能更高效地完成。**一个代理会收集有关资源使用情况和可用性的数据，从而能够快速找到具有足够资源的节点**。
- 核心本质：打破纯对等⽹络的对称性，引⼊特殊节点提升性能：
	- 索引服务器（index servers）：改善非结构化 P2P 系统的搜索性能。
	- 代理服务器（brokers）：收集资源使用和可用性数据，快速找到资源充⾜的节点。Collects data on resource usage and availability can quickly find a node with enough resources.
![[Pasted image 20251227153521.png]]
#### 超级节点

- 这些维护索引或充当代理的特殊节点被称为 **超级节点（super peers）**。超级节点通常在自身形成一个 P2P 网络，构成一个层次化结构。在这种设置中，每个普通节点（现在称为**弱节点 weak peer**）都会连接到一个超级节点。弱节点的所有通信都通过其超级节点完成。These special nodes that **maintain an index or act as brokers** are called super peers. Super peers are **usually organized in their own peer-to-peer network, creating a hierarchical structure.** In this setup, every regular peer, now called a **weak peer**, connects to a super peer. **All communication for the weak peer goes through its super peer**
##### 弱节点与超级节点的关联

- 通常情况下，当一个弱节点（weak peer）加入网络时，它会连接到某一个超级节点（super peer），并一直保持连接直到它离开网络。超级节点通常被认为是稳定且高度可用的。为了处理潜在的不稳定性，会采用备份机制，例如让每个超级节点成对出现，并让弱节点同时连接到这两个超级节点。
- 然而，固定地绑定某个超级节点并不总是最理想的。在文件共享网络中，例如，一个弱节点更适合连接到拥有该弱节点感兴趣文件索引的超级节点。这可以提高超级节点知道文件存放位置的概率。有一种机制允许弱节点在发现更好的超级节点后更换连接对象。成功帮助完成查找操作的超级节点会比其他节点获得更高的优先级。
## 文件共享

**==理解BitTorrent通过“以牙还牙”策略鼓励节点同时上传和下载。==**

#### 核心原则（查找文件 F）

1. 在全局目录中查找文件，获取种子文件（torrent file）。
2. 种子文件包含跟踪器（tracker）的引用，跟踪器是记录拥有文件 F 片段的活跃节点列表的服务器。
3. 节点加入节点群（swarm），免费获取一个文件片段，之后通过与其他节点交换片段获取完整文件。

#### 工作机制

- 其核心理念在于，当你需要某个文件时，你可以从其他用户处下载该文件块。一旦你拥有了所有这些文件块，它们就会被整合成完整的文件。（you download chunks of it from other users. Once you have all the chunks, they are assembled into the complete file）BitTorrent的设计初衷是促进协作。在大多数文件共享系统中，大量用户仅会下载文件而不会回馈任何内容。为防止这种情况发生，**==BitTorrent仅允许你下载文件的前提是，你也需向他人上传内容==**。**==BitTorrent only allows you to download a file if you're also uploading content to someone else==**

- 想要下载⼀份⽂件，你首先需要进⼊⼀个全局索引⽬录，这类⽬录通常可以在知名⽹站中找到。该⽬录中存放着种子文件（torrent ⽂件） 的访问链接，这类⽂件包含了下载指定⽂件所需的全部信息。⼀份种⼦文件中会包含⼀个追踪器（tracker） 的链接⸺追踪器是⼀台服务器，其作⽤是记录并追踪 哪些节点（用户）持有⽬标文件的文件块。这些处于活跃状态的节点，也正在下载这份⽂件。网络中有许多追踪器服务器，但通常每份⽂件（或每组⽂件）仅对应⼀个追踪器。

- 一旦你找到了拥有所需数据块的节点，你自己也就成为了一个活跃节点。此时，你必须通过提供他人尚未拥有的文件数据块来协助他人。**==Once you find the nodes with the chunks you need, you become an active node yourself. At this point, you must help others by providing chunks of the file that they don't have yet==**. 这里有一条简单的规则来确保这一过程得以实施：**如果节点P发现节点Q的下载量大于其上传量，那么P 便可减缓向 Q发送数据的速率( ==if Node P notices that Node Q is downloading more than it's uploading, P can slow down the rate at which it sends data to Q==)**。只要P有可从Q处获取的内容，这一系统便能良好运转。正因如此，节点往往能够获得对其他众多节点的引用，从而为其交换数据提供了更多机会。

![[Pasted image 20251227153807.png]]
## 缓存

**==了解协作式缓存中代理之间可以相互通信。==**
### 协作缓存

在协作式或分布式缓存中，当某个 Web 代理（proxy）本地缓存未命中（cache miss）时，它会先向**邻近的代理缓存**查询这些代理是否保存了所请求的文档。**when a web proxy has a cache miss, it first checks with neighboring proxies to see if they have the requested document**

如果所有邻近代理都没有该文档，那么该代理才会将请求转发给负责该文档的 Web 服务器。

这种方法通常用于**属于同一组织或机构的 Web 缓存**。belong to the same organization or instituition.

![[Pasted image 20251227191923.png]]

协作式缓存比较适合**相对较小的客户端群体 relatively small groups of clients**，例如成千上万的用户。然而，这些用户群也完全可以使用一个单独的代理缓存来服务，因为单一代理缓存的通信和资源开销更低。

==**协作式缓存的效果非常依赖客户端的访问需求模式。the effectiveness of cooperative caching depends heavily on client demands.**==
#### 与分层式缓存（Hierarchical Caching）的比较

- 与分层缓存机制的比较:相较于分层和协作式缓存机制，两者之间存在一些取舍之处。
- 例如，由于协作式缓存通常是通过高速链路连接的，因此获取文档所需的时间远短于使用分层缓存机制的情况。此外，与分层缓存相比，协作式缓存的存储需求也相对较为宽松。

# 7、分布式系统理论与挑战
## 分布式计算问题

**==理解孤儿计算是指客户端崩溃后，服务器仍在执行请求所导致的计算任务。==**

如果客户端向服务器发送了请求，但在收到回复之前崩溃了，会发生什么？  
在这种情况下，服务器仍然继续执行这个任务，但已经没有客户端等待结果了。  
这一部分继续执行的工作称为**孤儿计算**。

What happens **if a client sends a request to the server and then crashes before getting a reply**? **In this case, the server is still doing its work, but no one is waiting for the result**. This remaining work is called an **==orphan computation==**. 

孤儿计算会带来多个问题：
- 它们会浪费服务器的处理能力；waste processing power
- 它们可能会一直占用文件锁或其他关键资源；lock files or tie up other valuable resources
- 更麻烦的是，如果客户端重启后再次发送相同的请求，却意外收到孤儿计算返回的旧结果，就会造成严重混乱。things can get really confusing. This is basically about **ensuring at-most-once semantics and restoring the client to its pre-crash state**.

这个问题本质上与确保“最多一次执行语义”（at-most-once semantics）以及把客户端恢复到崩溃前状态有关。

一种解决方案叫做**检查点（checkpointing）**：在发送请求前保存客户端状态，如有需要再恢复。
# 大题1

## 1. continuous consistency的目的
### 目的

Continuous consistency 的核心目的，是允许副本之间在数值、时效性或更新顺序上存在可控的偏差，从而减少强一致性带来的高昂协调成本，同时满足应用对一致性的实际需求。

The core purpose of continuous consistency is to **allow controllable deviations between replicas in terms of numerical values, timeliness, or update order.** This reduces the high coordination costs associated with strong consistency while meeting the practical consistency requirements of applications.
## 2. 量化观测手段
### 量化方式

1. 副本之间可能在它们的**数值 numerical value**上有所不同。
	这对具有数值意义的数据特别有用，比如股票市场价格。  
	例如，一个应用可能规定两个股票价格副本之间的差异不能超过 **0.02 美元（绝对误差）**，或者不能超过 **0.5%（相对误差）**。  
	因此，如果股票价格上涨，而其中一个副本立即在这个误差范围内完成了更新，那么这些副本仍然被认为是一致的。
	这也可以指某个副本应用的更新次数与其他副本不同。例如，某个 Web 缓存可能还没有收到来自服务器的最新一批更新操作。
2. 副本之间可能在数据的相对**陈旧程度（staleness）** 上有所不同。
	这指的是副本中数据的“过时程度”。一些应用可以接受一定程度的旧数据，只要不太旧即可。  
	比如，天气报告在几个小时内依然是准确的。主服务器可能会及时接收到更新，但只会偶尔把更新发送给副本。
3. 副本之间在“**已执行更新操作(performed update operations)** 的数量和顺序”上可能存在差异。
	有些应用可以**容忍不同副本之间的更新顺序不同**，只要这些差异在一定范围内即可。
	可以把这些更新想象成：在等待所有副本之间达成全局一致之前，先临时应用到本地副本上。有些更新可能需要在成为最终结果之前被回滚并以不同的顺序重新应用。
	相比数值偏差或陈旧度偏差，**更新顺序偏差**通常更难理解和处理。

## 3. 用户中心的一致性模型如何实现；

**示例: 移动用户的一致性

考虑一个通过笔记本访问的分布式数据库。假设你的笔记本作为数据库的前端
- 在位置A，您通过读取和更新操作访问数据库。
- 在位置B，你继续工作，但除非你访问与位置A相同的服务器，否则你可能会检测到不一致:
	- 你在A处的更新可能尚未传播到B
	- 你可能读取的条目比A处可用的更新
	- 你在B处的更新最终可能与A处的更新冲突

**你真正想要的只是你在A处更新和/或读取的条目，在B中应保持与你在A处离开时相同的状态。在这种情况下，数据库将对你而言显得一致。**
## 4. 给出Replica Operation Ordering and Sequential Consistency的配置场景，分析replica按照某种给定的步骤执行各自操作，能否保证sequential consistency
### 顺序一致性，即所有进程看到相同的操作顺序，但不一定是实时顺序 Sequential Consistency

**定义**：任何执行的结果，==**都等同于所有进程的操作按某种顺序依次执行的结果，且每个独立进程的操作，都按照其程序规定的顺序出现在该序列中。**==

**The result of any execution is the same as if the operations of all processes were executed in some sequential order**, and the operations of each individual process appear in this sequence in the order specified by its program.

这意味着数据存储的表现，就好像所有读写操作都按某个特定顺序发生一样。即便进程位于不同机器上，每个进程都会按照自身程序设定的序列看到这些操作。关键在于，所有进程都会以相同的顺序看待这些操作 —— 而这些操作**在实际时间中的发生顺序并不重要**。

It is when a data store behaves as if all read and write operations happened in a specific order. Each process sees these operations in the sequence set by its own program, even if they're on different machines. What's important here is that all processes view the operations in the same order, but **it doesn’t matter when they actually happened in real-time.**

**示例分析**：

- 符合顺序一致性的存储：进程$P1$先写$x$为$a$，进程$P2$后写$x$为$b$，进程$P3$和$P4$均先读取到$b$、后读取到$a$，所有进程感知的写操作顺序一致。
- 不符合顺序一致性的存储：$P3$感知到$x$先变为$b$再变为$a$，而$P4$先读取到$a$再读取到$b$，进程间操作顺序感知不一致，违反顺序一致性。
![[Pasted image 20251227162511.png]]

![[Pasted image 20251228140810.png]]
- **Prints**：按 print 实际发生的时间顺序 拼接
- **Signature**：按 固定进程顺序 `P1 → P2 → P3` 拼接

顺序一致性是一种至关重要的模型，因为在开发并发应用程序时它是最易于理解的。当多个程序同时处理共享数据时，它能够符合我们的预期。然而，实现这一型并非易事。

考虑两个变量x和 y。如果 P1从x中读取值为'a’，而 P2 从y中读取值为'b’，乍看之下似乎并无不妥。但在考虑每个过程的执行顺序时，却无法对x和y上的这些操作进行排序，以使两次读取结果保持一致。这意味着 P1和 P2 进行的操作之间难以匹配，表明顺序一致性在不同数据项之间并不总能良好结合。正如表格所示，操作存在**多种可能的排序方式，进而导致多种结果的产生。**

![[Pasted image 20251228142030.png]]

![[Pasted image 20251228143211.png]]
![[Pasted image 20251228143222.png]]
![[Pasted image 20251228143555.png]]
![[Pasted image 20251228143603.png]]
![[Pasted image 20251228143610.png]]

#### 线性化

**假定操作会在其开始与结束之间的某个时刻瞬间发生**。在我们的示例中，线性化意味着每次写入操作的影响应在其执行时间内显现出来。例如，如果W2(y)b在W1(y)a开始之前完成，那么y最终结果应为'a’。反之亦然，如果W1(x)a在W2(x)b开始之前完成，那么x最终结果应为'b’.
阴影区域表示的是每项操作执行期间的时间段。线性化性原则指出，一项操作的影响应发生在该阴影时间段内的某个位置。这实质上意味着一旦写入操作完成，其结果便应扩散至其他数据存储中。
W2 (y) b 完成于 W1 (y) a 启动之前，因此 y 的最终值为 a。同理，W1 (x) a 完成于 W2 (x) b 启动之前，因此 x 的最终值为 b。在多核系统中实现线性一致性（linearizability）可能会显著降低运行速度，但会大幅简化编程流程。因此，这其中存在性能与简洁性之间的权衡。
![[Pasted image 20251228142340.png]]
如果 **一个写操作完全结束** 以后，另一个操作再开始，那么后面这个操作应该看到写操作的效果。
## 5. Read-Your-Writes Consistency in Browser Caching 如何实现。
### 读己所写 Read your Writes

**==读己之写一致性，即确保客户端总能读到自己已写入的最新数据**==

**定义**：每当一个进程对数据项$x$进行写入操作时，该进程随后进行的任何读取操作都会看到由该写入操作所导致的更改，无论读取操作发生在何处。换言之，在你写入内容后，你应该能够立即读取到内容，并且能看到你所进行的更改。

进程 P1 向 x 写入数据（记为 W₁(x₁)），随后在另一个位置读取 x。读己写一致性确保这次读取能看到写入的效果。这一点可通过 W₂(x₁;x₂) 体现，它表示进程 P2 基于 x₁创建了 x 的新版本。

![[Pasted image 20251228153725.png]]

若进程 P2 与 x₁同时创建了 x 的版本（即 W₂(x₁|x₂)），意味着 P1 在 L₁的写入尚未同步到 L₂。此时 P1 读取 x₂时，无法看到自己在 L₁执行的写入所做的更改。
![[Pasted image 20251228153745.png]]

When a client reads from a server, that server has seen all the writes in the client’s write set. 
One way to do this is for the server to fetch these writes from other servers before performing the read operation, although this can slow down response time. 
Alternatively, the client-side software can search for a server where all the necessary writes have already been completed.
When client C wants to read at server S, C passes its write set. S can pull in any updates before executing the read operation, after which the read set is updated.

当客户端从服务器读取数据时，该服务器已接收到客户端写入集合中的所有写入操作。实现这一过程的方法
1. **服务器**在执行读取操作前**从其他服务器获取这些写入信息**，尽管这可能会延长响应时间。
2. **客户端**软件可以**查找**所有必要**写入操作均已完成的服务器**。当客户端C希望在服务器S上进行读取时，C会传递其写入集合。S可以在执行读取操作前获取任何更新信息，随后再更新读取集合。

通俗点讲就是：

**为了保证你能读到自己写过的东西，浏览器或客户端会把你写过的内容列表带给服务器，服务器会根据这个列表把自己落下的更新补齐（或者客户端直接找最更新的服务器），然后再把正确的内容返回给你。**

![[Pasted image 20251228150150.png]]

浏览器或代理服务器为了加速访问，会把网页、图片、CSS 等内容缓存起来。  
但缓存可能过时（比如网页作者刚更新过内容），如果仍把旧版本返回给用户，就会造成：
- 看不到最新网页
- 作业被老师改了但你看到旧版本
- 自己更新的网站却看不到自己的更新
因此需要 **Web-cache consistency（缓存一致性机制）** 来防止返回过期内容。

![[Pasted image 20251228150502.png]]
![[Pasted image 20251228150547.png]]

# 大题2
## 1. Stateless application relying on replication to tolerate faults的场景下，replica可能有故障、可能有延迟、可能无法发送响应给主节点等情况，需要多少replica能容纳k个错误

**==掌握不同故障模型下，为容忍k个故障所需的最小副本数N，如拜占庭故障（N≥3k+1）；崩溃/遗漏故障（N≥k+1）。==**

### 一、错误类型与对应故障分类

首先明确副本的三种情况对应的故障类型，这是确定副本数量的前提：

1. **副本有故障**：对应**停止故障（Halting Failures）**，如进程崩溃、硬件故障，副本完全停止工作。
2. **副本有延迟**：对应**性能退化 / 时序故障（Timing Failure）**，副本未停止但响应缓慢，属于 “部分失效故障”。
3. **副本无法发送响应给主节点**：对应**遗漏故障（Omission Failure）** 中的 “发送遗漏”，副本正常运行但无法回传响应。

注：以上三种均属于 “非恶意故障”，无错误输出（区别于拜占庭故障），课件中统一归类为 “停止类故障相关的部分失效”，容错策略一致。

### 二、副本数量计算规则

无状态应用的复制容错遵循 “冗余覆盖故障” 逻辑，两类核心场景的副本数量要求：

- 对于停机类故障（Halting failures：崩溃 / 遗漏 / 时序故障）
	需要 **k + 1 个成员**，因为
	- 这些故障不会产生错误结果（即不会返回错误答案）
	- 因此，只要有 **一个正确的成员存活**，它的结果就足够了
	如果其中有 k 个成员停机，那么剩下那个正常成员的返回结果就可以使用。

- 对于任意故障（Arbitrary failures）
	需要 **2k + 1 个成员**，原因是：
	- 我们必须通过 **多数表决（majority vote）** 来获得正确结果
	- 在最坏情况下，k 个故障成员可能会一致返回相同的错误结果
	- 但剩下的 **k + 1 个成员** 会返回正确结果
	- 客户端可以信任多数派的结果
	- 重要假设（Important assumptions）
		- 所有成员都是 **完全相同的（identical）**
		- 所有成员以 **相同顺序处理命令**
![[Pasted image 20251228151211.png]]
## 2. 如果有replica被compromised（副本被攻破 / 沦陷），需要多少replica实现拜占庭容错？

### 对于真正的拜占庭故障

**核心问题：**

考虑进程间通信不⼀致的进程组，可能出现 “错误转发 improper forwarding”（如进程 P₂转发与应转发的值或操作不同）或 “消息不⼀致 different messages”（如进程 P₁向不同进程发送不同操作）等情况。这些不⼀定是恶意⾏为，可能是遗漏或执⾏故障。
![[Pasted image 20251228170934.png]]

**需要>=3k+1个成员**，原因是：

- **在拜占庭环境中，恶意节点可以向不同诚实节点发送不同消息。** 这意味着：
	- 恶意节点可以让诚实节点看到不同的“视图”
	- 每个诚实节点可能以为自己看到的是多数派
	- 但这些“多数派”可能互相矛盾（因为恶意节点在背后操控信息）
	- 
假设 k = 1，那么：

- 总节点数 = 2k+1 = 3
- 恶意节点 = 1
- 诚实节点 = 2

节点：A（诚实）、B（诚实）、C（恶意）
恶意节点 C 可以：
- 给 A 发送值 = X
- 给 B 发送值 = Y

于是：

- A 看到的“多数派”（它自己 + C）是 X
- B 看到的“多数派”（它自己 + C）是 Y

A 与 B 都以为自己多数派是正确的，最终系统分叉了

所以，仅有多数诚实节点 **不足以** 保证一致性。

我们必须保证：任何两个多数派之间都必须有至少一个共同的诚实节点。

假设总节点数为 N，要容忍 k 个拜占庭节点。

为了保证多数派之间有交集：多数派大小必须 > N/2

设每次投票的多数派大小为 M。 我们需要两个多数派 A 和 B 满足：|A ∩ B| ≥ 1（并且这个节点必须是诚实的）

为了保证交集必然包含诚实节点，必须满足：

- M > N/2
- 诚实节点 ≥ 2k+1（因为 N = 诚实节点 + k）

现在让我们设置：N = 3k + 1
那么诚实节点数 = N – k = 2k+1
多数派大小 = M = 2k+1（因为 M > N/2 = 1.5k + 0.5）
现在检查两个多数派是否必然有重叠：

- 多数派 A = 2k+1 个节点
- 多数派 B = 2k+1 个节点

总节点数只有 3k+1 个，不论怎么选，两个 2k+1 大小的集合必然至少有   (2k+1) + (2k+1) – (3k+1) = k+1 个交集节点。而其中至少 k+1 – k = **1 个节点必然诚实**。
## 3. 如果availability比consistency重要，根据CAP理论，系统在出现partition时应如何运行保证availability?（仔细看看容错理论、Paxos等知识）

### CAP定理

**==理解其核心思想，即在分布式系统中，一致性、可用性和分区容错性三者不可兼得。==**
#### 核心内容

任何提供共享数据的网络系统最多只能满足以下三个特性中的两个：
- C（一致性，Consistency）：共享复制数据项呈现为单一的最新副本。a shared and replicated data item appears as a single, up-to-date copy
- A（可用性，Availability）：更新操作最终将被执行。updates will always be eventually executed
- P（分区容错性，Partition tolerance）：容忍进程组分区（如网络故障）。Tolerant to the partitioning of process group, like network failures

### 1. CAP 视角：P 分区发生时只能选 C 或 A

CAP 里：

- **P（Partition tolerance）**：分区一旦发生，你没得选，一定要面对。
- 这时就只能在 **C（Consistency）** 和 **A（Availability）** 之间选一个。

**Availability 优先（AP 系统）** 的意思是：

> 只要某个副本/节点还活着、还能收请求，就必须给客户端一个响应，而不是说“我去问一下别的分区，问不到就一直卡着”。

这直接和 Paxos 的“要多数派才能提交”冲突
### 2. 容错理论、Paxos 的默认选择：CP 不是 AP

在 **Paxos / Raft** 这种共识协议里：

- 要提交一个写请求，必须得到 **多数派（> N/2）节点的确认**。
- 一旦网络分区：
    - **有多数的一侧** 可以继续（leader 在这边时）
    - **没有多数的一侧** 必须停掉写（甚至读也要停，取决于实现）

这样做保障了**线性一致性（strong consistency）**，但牺牲了另一边的可用性 ——  
这就是典型的 **CP 系统**（Consistent + Partition tolerant）。

所以如果你说：在分区时我还要保证“整个系统”对所有客户端都是 available

那就 **不能继续用“要全局多数”的那套 Paxos 思路**。

### 3. 想要 Availability：分区时如何运行？

**核心原则：**

> **在每个分区内部都可以单独提供服务，不再强行等待其它分区的回答。**

### 具体应该怎么做？

1. **每个分区都允许本地读写**
    - 客户端连到哪个分区，就在那个分区完成 read/write。
    - 不要求跨分区的同步确认（否则就会阻塞 —— 失去 availability）。
2. **写操作采用异步复制**
	- 写先在本分区落盘并返回成功；
    - 之后再通过后台机制把更新传播到其他分区（等网络恢复）。
3. **读操作只依赖本分区的数据**
    - 不做“全局最新”的强校验；
    - 通常保证的是 **eventual consistency**：没有新的写入后，所有副本最终会趋于一致。
4. **分区恢复后进行冲突解决（reconciliation）**
    - 比如使用：
        - 时间戳 / “last write wins”
        - 版本向量（vector clock）
        - 应用层 merge 逻辑（购物车合并、计数相加等）
    - 这一步是**事后补偿一致性**，而不是在 partition 当时阻塞写入。

### 4. 和 Paxos 的关系：怎么办？

如果你已经在用 Paxos / Raft，又想在分区时继续服务**所有**分区，那你基本上要：

- **不再强制所有写都经过 Paxos 共识**  
    （否则少数派分区一旦拿不到多数就不能提交写）
- 或者 **在每个分区内部再跑一个“局部 Paxos”**，把每个分区当成一个独立系统：
    - 这相当于你承认分区期间产生了“多个不同的历史日志”，
    - 分区恢复后再把这些历史“合并/重放”，仍然需要冲突解决。

严格来说：

> **真正的 Paxos 共识协议是 CP 的，而不是 AP 的。  
> 想要 AP，就得在分区时“绕过共识”，转向“最终一致 + 冲突合并”的路线。**