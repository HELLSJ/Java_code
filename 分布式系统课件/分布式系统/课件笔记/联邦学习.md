## 目录

1. 什么是联邦学习
2. 基准算法：联邦平均（Federated Averaging）
3. 联邦学习面临的部分挑战

## 1. 什么是联邦学习

- 机器学习的标准设置是在紧密集成的系统中处理集中式数据集。
- 但在现实世界中，数据通常分散在多个参与方之间。
![[Pasted image 20251110220934.png]]
### 数据集中式处理的问题

- 数据传输成本过高。
	- 自动驾驶汽车每天预计会产生数太字节（TB）的数据，
	- 部分无线设备的带宽和电量有限。
- 数据敏感性问题突出。
	- 公众对数据隐私的关注度不断提高，相关法规也日益完善
	- 掌控数据能在商业和研究中带来竞争优势。
- 本地数据集存在局限。
	- 本地数据集可能规模过小，导致预测性能不佳（如过拟合）或研究结果缺乏统计显著性（如医学研究）；
	- ![[Pasted image 20251110221046.png]]
	- 可能存在偏差，无法代表目标分布。
	- ![[Pasted image 20251110221055.png]]

### 联邦学习的定义与核心流程

联邦学习（FL）旨在**实现机器学习模型的协同训练**，同时**保持数据的去中心化**。

![[Pasted image 20251110221119.png]]
1. **初始化模型。**
![[Pasted image 20251110221129.png]]
2. **各参与方利用本地数据集对模型进行更新。**
![[Pasted image 20251110221138.png]]
3. **参与方共享本地更新结果以进行聚合。**
![[Pasted image 20251110221145.png]]
4. **服务器对更新结果进行聚合后，将聚合后的模型反馈给各参与方。**
![[Pasted image 20251110221150.png]]
5. **参与方更新本地模型副本，重复上述流程迭代训练。**
![[Pasted image 20251110221313.png]]

- 最终目标是让训练出的模型**性能接近集中式训练方案**（理想情况下），或至少**优于各参与方单独训练的模型**。

### 与分布式学习的区别

| 特征   | 分布式学习         | 联邦学习                                                                |
| ---- | ------------- | ------------------------------------------------------------------- |
| 数据存储 | 集中存储（如数据中心）   | 自然分布，本地生成                                                           |
| 核心目标 | 提升训练速度        | 协同训练，保持数据去中心化                                                       |
| 数据分布 | 通常随机均匀分配给工作节点 | 非独立同分布not independent and identically distributed（non-i.i.d.），存在不平衡 |
| 额外挑战 | 无特殊隐私约束       | 需满足隐私要求、应对参与方可靠性 / 可用性有限问题、抵御恶意参与方攻击                                |
### 联邦学习的分类

#### 跨设备与跨机构联邦学习

**跨设备联邦学习：**
![[Pasted image 20251110221443.png]]
- 参与方数量庞大（最多达 10¹⁰个）
- 每个参与方的数据集规模小（可能仅含 1 条数据）
- 可用性和可靠性有限
- 部分参与方可能存在恶意行为。

**跨机构联邦学习：**
![[Pasted image 20251110221452.png]]
- 参与方数量为 2-100 个
- 每个参与方的数据集规模中到大
- 参与方可靠性高、几乎始终在线
- 通常为诚实参与方。

#### 服务器协调式与完全分布式联邦学习

**服务器协调式联邦学习：**
![[Pasted image 20251110222539.png]]
- 采用服务器 - 客户端通信模式
- 具备全局协调和全局聚合能力
- 但服务器是单点故障源，可能成为性能瓶颈。

**完全分布式联邦学习：**
![[Pasted image 20251110222627.png]]
- 采用设备间直接通信模式
- 无全局协调，仅进行本地聚合
- 天然适用于大规模设备场景。

### 联邦学习的发展现状

- 2016 年，谷歌研究人员首次提出 “联邦学习” 术语；2020 年上半年，相关学术论文已超 1000 篇。
- 已有多家企业和研究机构实现联邦学习的实际部署，多款开源库正在开发中，包括 PySyft、TensorFlow Federated、FATE、Flower、Substra 等。
- 联邦学习是一门多学科交叉领域，涉及机器学习、数值优化、隐私与安全、网络、系统、硬件等多个方向。

## 2. 基准算法：联邦平均（Federated Averaging）

### 问题设定

- 假设有 K 个参与方（客户端），每个参与方 k 持有包含 $n_k$ 个数据点的数据集 $D_k$。
- 令 D=D₁∪…∪DK 为联合数据集，n=∑ₖnk 为数据点总数。
	- 目标是求解优化问题：minθ∈Rp F (θ; D)，其中：    
	![[Pasted image 20251110222752.png]]
	![[Pasted image 20251110222800.png]]
- θ∈Rp 表示模型参数（如逻辑回归或神经网络的权重），该设定适用于各类基于经验风险最小化的机器学习问题。

### 联邦平均算法（FedAvg）

#### 服务器端算法

![[Pasted image 20251110222901.png]]

#### 客户端更新函数（ClientUpdate）

![[Pasted image 20251110222910.png]]

### 算法特性与实验结果

- 当 L=1 且 p=1 时，联邦平均等价于经典并行随机梯度下降（SGD），每一步都会聚合更新并同步模型。
- 当 L>1 时，每个客户端在通信前会执行多次本地 SGD 迭代。

- 实验结果（基于 CIFAR-10 数据集）显示：
	- ![[Pasted image 20251110222939.png]]
    1. **L>1 的联邦平均算法可减少通信轮数**，而通信轮数通常是联邦学习（尤其是跨设备场景）的性能瓶颈。
    2. 与大批次并行 SGD 相比，联邦平均在泛化性能上更具优势。
    3. 对于独立同分布（i.i.d.）数据，联邦平均可保证收敛到最优模型，但在强非独立同分布场景下会出现收敛问题。

### 完全分布式随机梯度下降

#### 算法设定

- 我们可为完全去中心化场景设计类似FedAvg的算法，该场景下各方无需依赖服务器进行更新聚合。
- 参与方构成连通无向图 G=({1,…,K}, E)，边 {k,l}∈E 表示参与方 k 和 l 可交换消息。
- W∈[0,1]ᵏˣᵏ为对称双随机矩阵，当且仅当 {k,l}∉E 时，Wₖₗ=0。
- 对于各参与方的模型集合 Θ=[θ₁,…,θK]，邻域聚合公式为：![[Pasted image 20251110223105.png]]

#### 完全分布式 SGD 算法（参与方 k 执行）

![[Pasted image 20251110223117.png]]
- 算法交替执行本地更新和本地聚合，多次本地迭代等价于在部分轮次中设置 W (t)=Iₙ（单位矩阵）。
- 收敛速度取决于网络拓扑，连通性越强，收敛速度越快。

## 3. 联邦学习面临的部分挑战

### 客户端偏移问题

- 当本地数据集为非独立同分布时，联邦平均算法会出现客户端偏移现象。
- 为避免偏移，需减少本地更新步数或降低学习率，但这会影响模型收敛速度。
![[Pasted image 20251110223140.png]]
### 个性化模型的联邦学习

- 非独立同分布数据的训练难度大、速度慢，因为**各参与方的模型优化方向存在差异**。
- 若数据分布差异显著，训练适用于所有参与方的单一模型可能需要极多参数。
- 解决思路：不再要求所有参与方使用统一模型（“一刀切”），而是允许每个参与方 k 学习个性化模型 θₖ，通过设计目标函数实现参与方间的协同。


