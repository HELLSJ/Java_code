## 第一部分：摘要

Federated Learning (FL) is a new way of doing distributed machine learning. It allows multiple devices or organizations to work together to train a shared model **without sharing their original data**. Unlike traditional centralized training, where all data is uploaded to one server, FL lets each client train locally on its own dataset and only send model updates to a central server. This method was first introduced by Google to deal with privacy concerns and reduce communication costs in large-scale machine learning. By keeping data on local devices and only sharing updates, FL achieves collaborative learning while protecting user privacy.

A Federated Learning system mainly consists of **a central server** and **multiple distributed clients**. The server coordinates training and aggregates model updates, while clients perform local training on their own data. Training happens in several communication rounds. At the start of each round, the server sends the current global model to some clients. Each selected client trains it for a few epochs on local data, then sends the updated parameters back. The server then averages these updates, weighted by how many samples each client has. This process, called **Federated Averaging (FedAvg)**, is the core of Federated Learning. Over time, the global model improves, but raw data always stays on the clients. This keeps data private, reduces communication, and allows distributed training to happen securely.

One of the main advantages of FL is its **privacy protection**. Because data never leaves the device, the chance of data leakage is much lower than in centralized systems. The information exchanged between clients and server only includes model updates, which are less sensitive than the raw data itself. McMahan et al. suggested that FL could become even safer by combining it with **differential privacy** and **secure multi-party computation**, so that individual information cannot be inferred from updates.

However, Federated Learning still faces a key problem — **communication efficiency**. Many devices have limited network bandwidth or unstable connections, so frequent synchronization is costly. The FedAvg algorithm solves this by letting clients train for several local steps before communicating with the server. This greatly reduces how often updates are sent — sometimes by ten to one hundred times compared to traditional distributed stochastic gradient descent (SGD). As a result, more computation happens locally, and less data needs to be transmitted, achieving a good trade-off between accuracy and efficiency.

Another important challenge is **data heterogeneity**, also called **non-IID data**. In FL, each client’s data can be very different from others. For example, text typed by different users or photos taken by different devices may vary a lot. These differences can cause local models to update in different directions, making it harder for the global model to converge. McMahan and colleagues showed that FedAvg still performs well even with non-IID data, though training speed and accuracy can be affected by how uneven the data is.

In summary, Federated Learning is a privacy-preserving and efficient approach for collaborative model training. It allows models to improve without gathering sensitive data in one place. Through the FedAvg algorithm, McMahan et al. (2017) proved that deep learning can be done effectively in decentralized settings with limited communication. Despite challenges like data imbalance and unstable networks, FL remains a promising method for future machine learning systems that need to protect user privacy while maintaining strong performance.
## **第二部分：集中式学习基准（15 分）**

### **代码设计说明**

在集中式学习部分中，我设计并实现了一个基于卷积神经网络（CNN）的基准分类模型，用于对 Fashion-MNIST 数据集中的图像进行识别与分类。该模型在单一中央服务器上运行，所有数据集中存储并统一训练，以作为联邦学习实验的性能对照。模型结构包括两层卷积和两层池化层，随后是一个包含 512 个神经元的全连接层和输出层（10 类分类），使用 ReLU 激活函数。优化算法采用 Adam，学习率设置为 5e-4，损失函数为交叉熵。数据集通过 PyTorch 的`torchvision.datasets.FashionMNIST`进行下载，并分为训练、验证与测试三部分。训练过程中记录每个 epoch 的训练损失与验证准确率，并通过自定义函数 `find_converged_step` 判断模型的收敛轮次：当连续三轮准确率提升小于 0.5% 且损失变化小于 0.01 时，认为模型达到收敛状态。最终结果与曲线均自动保存为 CSV 与 PNG 文件，为后续联邦学习对比提供基准参考。

### **实验结果分析**

- Fashion-MNIST 数据集在集中式 CNN 模型下收敛迅速，在第 21 个 epoch 后准确率趋于稳定；
- 模型最终的验证准确率达到 **92.3%**，测试集准确率同样为 **92.3%**；
 ![[learning_curves_acc 1.png]]
- 验证准确率曲线显示在前 15 轮内提升明显，之后进入小幅波动状态；
 ![[learning_curves_loss 1.png]]
- 损失曲线呈现典型的下降趋势，训练损失逐步收敛至 0.02 左右，而验证损失在 20 轮后呈现波动上升，表明模型达到最优点并出现轻微过拟合趋势；
- 整体表现说明模型设计合理，能够在集中式场景中有效学习特征并达到高精度。

## **第三部分：联邦学习实现（30 分）**

### **代码设计说明**

在第三部分的代码设计中，我首先实现了 **模拟客户端与数据分区**。程序通过 `partition_iid` 和 `partition_non_iid` 两个函数来控制数据的划分方式。在 IID 模式下，所有训练样本被随机打乱后平均分给各个客户端，保证每个客户端的数据分布一致；而在 Non-IID 模式下，我在 `partition_non_iid` 中先统计每个类别对应的样本索引，再为每个客户端分配若干类别，从这些类别中抽取固定数量的样本，使得每个客户端只包含部分类别的数据，从而形成有偏分布。这种设计直观地反映了真实场景中数据分布不均的特点，也为后续的实验二提供了支持。

接下来在 **客户端模型训练** 部分，我设计了 `Client` 类来封装每个客户端的本地训练逻辑。每个客户端在初始化时根据自己的样本索引创建 `DataLoader`，并设置好损失函数和优化参数。通信开始前，服务器通过调用 `set_weights` 将全局模型权重下发到客户端，客户端随后使用 `train_local` 在自己的数据上进行若干轮训练。训练完成后，客户端通过 `get_weights` 把更新后的模型参数返回给服务器，以供后续聚合。这一过程模拟了现实中每个终端设备本地学习后再上传模型的机制。

在 **服务器端聚合** 环节，代码实现了一个 `fedavg` 函数，用于执行经典的 FedAvg 聚合算法。服务器在收集到各客户端上传的参数后，会按照每个客户端样本数量的比例对这些参数进行加权平均，生成新的全局模型权重。这种方法保证了拥有更多数据的客户端在全局模型更新中占据更大权重，从而更接近整体数据分布。

最后在 **通信轮次与训练循环** 的设计中，`run_federated` 函数将整个联邦学习过程组织为多轮通信循环。每一轮，服务器随机抽取部分客户端，广播当前全局模型参数，让客户端分别执行本地训练后返回结果。随后服务器聚合新的模型参数并在测试集上进行评估，将每轮的损失值与准确率记录在日志文件中。函数还会根据曲线变化判断收敛轮次并保存最优模型、曲线图及结果文件，方便后续在第四、第五部分进行实验分析与对比。整体设计结构清晰、模块独立，实现了从数据划分、客户端训练、参数聚合到通信控制的完整联邦学习流程。

## **第四部分：实验与分析（20 分）**

### **代码设计说明**

实验部分基于第三部分实现的联邦学习框架，通过自动化脚本实现了两类实验：**客户端数量影响实验**与 **IID vs Non-IID 对比实验**。

在 **客户端数量影响实验** 的实现中，脚本以固定参数（批量大小、学习率、通信轮次等）为基础，依次设置客户端数量为 5、10、20，并保持数据划分方式为 IID。每次运行后，程序记录每轮的通信精度与损失，并保存相应的日志与曲线。随后，程序将三组实验结果汇总到一张综合图中，展示随着客户端数量增加，模型收敛速度和最终精度的变化趋势。该部分的实现通过循环控制客户端数量参数和自动绘图函数完成，从而清晰体现出“客户端数量增加导致的通信复杂度上升与模型聚合误差加大”这一典型联邦学习现象。

在 **IID 与 Non-IID 对比实验** 的实现中，代码将客户端数量固定为 10，同时切换 `iid=True` 与 `iid=False` 两种模式来分别运行实验。对于 Non-IID 模式，脚本指定每个客户端仅持有两类样本数据，从而形成高度不均的分布。程序随后记录两种设置下的测试准确率与损失曲线，并在同一张图中对比绘制，直观反映数据分布一致性对联邦平均（FedAvg）算法稳定性的影响。通过这种统一化实验结构，程序实现了在相同模型与训练配置下的可复现实验，确保结果之间具有可比性与统计意义。

### **实验结果分析**

**实验 1：客户端数量影响**

![[exp1_clients_loss.png]]

![[exp1_clients_acc.png]]
- 当客户端数量为 5 时，模型收敛最快，50 轮内准确率达到 **91.8%**；
- 客户端数量增加到 10 时，准确率为 **91.7%**，收敛稍慢；
- 当客户端数量增至 20 时，准确率降至 **90.9%**，损失曲线下降更缓；
- 三种情况下的损失曲线均表现出平滑下降趋势，但客户端数量越多，波动越明显；
- 结果表明，客户端数量增加会引入模型聚合异质性，使得训练收敛速度与最终精度略有下降。

**实验 2：IID vs Non-IID**

![[exp2_iid_vs_noniid_loss.png]]

![[exp2_iid_vs_noniid_acc.png]]
- IID 模型在 45 轮左右收敛，准确率稳定在 **91.7%**；
- Non-IID 模型表现出严重震荡，准确率最高仅 **64.8%**；
- Non-IID 的测试损失在训练过程中持续波动，无法稳定下降；
- 实验证明，非独立同分布的数据分割会显著降低联邦平均算法的聚合稳定性与模型性能；
- 相比之下，IID 分布下的联邦模型几乎可与集中式模型持平。

## **第五部分：与集中式学习的性能对比（5 分）**

### **代码设计说明**

性能对比部分通过脚本自动扫描集中式训练日志和联邦学习的 summary 文件，读取各模型的最佳准确率与收敛轮次。若同名实验结果重复，程序自动保留准确率更高且收敛更快的版本，以避免冗余。最后将集中式与联邦模型的结果合并后，按准确率降序绘制柱状图，并在柱顶标注准确率百分比，在柱体内标注收敛轮次。绘图采用 Matplotlib，颜色区分集中式（绿色）、IID 联邦（蓝色）与 Non-IID 联邦（橙色），以便于对比模型性能与稳定性。

### **实验结果分析**

![[part5_results_bar_sorted_fixed.png]]
第五部分的结果综合展示了集中式学习与联邦学习在相同模型和数据集条件下的整体性能差异。从图中可以看出，集中式模型在准确率上依旧保持领先，但与 IID 联邦模型的差距非常小，说明当数据分布均匀时，联邦学习能够在不集中数据的情况下逼近集中式性能。这一现象体现了 FedAvg 算法的核心价值——在保护数据隐私的前提下实现接近全局最优的学习效果。

另一方面，Non-IID 模型的表现明显低于前两者，其准确率不仅下降近三成，还伴随训练曲线剧烈震荡，说明不同客户端之间的数据分布差异严重干扰了模型聚合过程。这种现象揭示出联邦学习的关键挑战之一：在实际场景中，设备间数据往往存在显著差异，如何在此条件下保持稳定收敛仍是研究重点。

值得注意的是，虽然联邦模型在收敛轮次上普遍较慢（约 40–50 轮），但其在大规模分布式环境中的通信与训练是并行进行的，因此在整体效率上仍具优势。从算法角度看，这种“精度换通信”的权衡符合分布式学习的基本规律。综合来看，本实验验证了联邦学习在 IID 条件下的有效性与在 Non-IID 条件下的脆弱性，也反映出模型一致性、客户端采样与聚合策略优化的重要性。

## **挑战与解决方案**

1. Non-IID 数据分布导致模型的全局参数更新方向不一致，使准确率曲线震荡严重。为此我降低学习率并保持较小批量大小以减缓不稳定更新。
2. 联邦与集中式实验产生的 JSON 文件可能重复，造成结果冲突，我在 Part4 加入“结果存在即跳过训练”逻辑，并在 Part5 自动去重。
3. 模型收敛判定标准不一致容易导致统计混乱，我统一使用 `find_converged_step` 函数以相同阈值判断收敛。
4. 为了保证结果可复现，我在每个模块中固定随机种子并关闭 CUDA 非确定性选项，从而确保多次运行结果一致。通过以上调整，系统性能和可靠性显著提升，最终实验曲线与理论分析完全一致。
## **参考文献（5 分）**

1. McMahan, H. B., et al. “Communication-efficient learning of deep networks from decentralized data.” _AISTATS_ (2017).
